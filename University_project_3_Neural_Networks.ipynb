{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabrinasforza/Portfolio/blob/main/University_project_3_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqEIutC0T2KZ"
      },
      "source": [
        "# Task 2: Predicting Student Grades \n",
        "\n",
        "## Abstract\n",
        "The second task of this neural network project consists of building a multi-class classifier to predict secondary school student performance. The data used to train the classifier models was collected from two Portuguese secondary schools in 2014.\n",
        "\n",
        "Firstly, a random forest classifier was used a baseline model. Then a multilayer perceptron was trained before building a 3-layer dense deep neural network to compare their performance. We then built two wide and deep neural networks with different architectures. \n",
        "\n",
        "## Main Findings\n",
        "\n",
        "It was found that several attributes within the dataset helped predict student performance more effectively. These consisted of previous grades, mother's education, aspiration to take up higher education, studytime, amount of failures and amount of absences.\n",
        "\n",
        "Using a reduced dataset, both a dense, deep neural networks and a wide and deep neural network with varying architecutre were highly effective as predicting student performance. The most effective model was the wide and deep neural network with (x), as it scored an F1 score of (X).  \n",
        "\n",
        "\n",
        "## Team Name: Group 7\n",
        "\n",
        "## Students: \n",
        "\n",
        "\n",
        "* DEWHIRST GREGOR | 202172038 \n",
        "* GRAY JAMES | 202181792 \n",
        "* LANCASTER THOMAS | 202191074 \n",
        "* SFORZA SABRINA | 202173415 \n",
        "* SMITH PADDY | 202171142 \n",
        "* THOMAS SAM KURIAKOSE | 202179307 \n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4o30PvPTqFX"
      },
      "source": [
        "# Model and Method "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "2mQb9qe73Mkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.linear_model import Perceptron\n",
        "!pip install -U tensorflow\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from tensorflow import keras\n",
        "!pip install -q -U keras-tuner\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import seaborn as sns \n",
        "\n",
        "# Keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Standard ML stuff\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD, FastICA\n",
        "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
        "\n",
        "# Oversampling of minority class 'Churn customers'\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "mMg8XGd73LMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae2afb3-6b98-4362-ce47-bb1b549bf9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "2.8.0\n",
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Processing and Feature Selection\n"
      ],
      "metadata": {
        "id": "u7-iKcHuYz3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing test set\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/test_task2.csv\"\n",
        "test = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "eVOXx9UX4vXR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "b3fae80f-89e9-4c43-b367-4371e3599256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5252a5dbfa0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#path = \"/content/drive/MyDrive/test_task2.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#test = pd.read_csv(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing train set \n",
        "\n",
        "path = \"/content/drive/MyDrive/train_task2.csv\"\n",
        "train = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "aE66wmQ_4vts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choosing predictors and targets \n",
        "x_test = test\n",
        "x_train = train\n",
        "y_train = train.pop('Grade')"
      ],
      "metadata": {
        "id": "FH16X9ce5dsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping id column from test and train sets \n",
        "x_train.drop('id', axis=1, inplace=True)\n",
        "x_test.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ucHkN5hn5hEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Describe Dataset\n",
        "\n",
        "The dataset consists of various information on students from two Portuguese schools, collected through surveys and questionnaires. In total, 33 attributes have been collected for each student, and there are (x) observations within the dataset. \n",
        "\n",
        "Some initial analysis through a heatmap shows a correlation of 0.6 between the amount of hours studied and the grade recieved, which is theoretically cogent. \n",
        "\n",
        "Furthermore, a negative correlation of -0,2 can be both seen with the amount of failures and the amount of of absences a student has.These are both to be expected as previous failure will likely reflect future performance and absences will likely determine how much content is being consumed. \n",
        "\n",
        "After, the grades distribution was checked, showing a positive skew. This shows that there is not a normal distribution amongst the student sample. "
      ],
      "metadata": {
        "id": "DD5mLJun4H1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exploring trends in the data\n",
        "\n",
        "corr = train.corr()\n",
        "sns.heatmap(corr) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oBSVlFb25PlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if student grades are evenly distributed\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "train[\"Grade\"].value_counts().plot(kind=\"bar\", \n",
        "                                  figsize = (8,5), color = \"darkviolet\")\n",
        "plt.title(\"Frequency of the classes of our Target variable\", size=20)\n",
        "plt.xlabel(\"Target Variable\", size = 16)\n",
        "plt.ylabel(\"Frequency\", size = 16)"
      ],
      "metadata": {
        "id": "YfqYe5Zd5URl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance\n",
        "\n",
        "A chi-squared feature selection is performed in order to ensure only relevant attributes are being used to train the classification model. This is because certain attributes may predict a students performance far better than others. Removing unwanted attributes, thus reducing the size of the dataset, can make it easier and quicker to train neural networks. However shrinking the amount of input variables could create problems of overfitting, as the model may not be able to generalise to other datasets. \n"
      ],
      "metadata": {
        "id": "k3NnTLnE5mkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All input variables have to be converted into ordinal variables first before feature importance can be performed.\n",
        "\n",
        "def prepare_inputs(x_train, x_test):\n",
        "    oe = OrdinalEncoder()\n",
        "    oe.fit(x_train)\n",
        "    X_train_enc = oe.transform(x_train)\n",
        "    X_test_enc = oe.transform(x_test)\n",
        "    return X_train_enc, X_test_enc\n",
        "\n",
        "    def prepare_targets(y_train):\n",
        "    le = LabelEncoder()\n",
        "    le.fit(y_train)\n",
        "    y_train_enc = le.transform(y_train)\n",
        "    return y_train_enc\n",
        "\n",
        "    def select_features(x_train, y_train, x_test):\n",
        "    fs = SelectKBest(score_func=chi2, k='all')\n",
        "    fs.fit(x_train, y_train)\n",
        "    x_train_fs = fs.transform(x_train)\n",
        "    x_test_fs = fs.transform(x_test)\n",
        "    return x_train_fs, x_test_fs, fs\n",
        "\n",
        "    X_train_enc, X_test_enc = prepare_inputs(x_train, x_test)\n",
        "y_train_enc = prepare_targets(y_train)\n",
        "x_train_fs, x_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)"
      ],
      "metadata": {
        "id": "sk1RibSH6b39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance scores are then calculated for each variable, and a bar plot is produced\n",
        "for i in range(len(fs.scores_)):\n",
        "    print('Feature %d: %f' % (i, fs.scores_[i]))\n",
        "# plot the scores\n",
        "plt.figure(figsize = (12,6))\n",
        "plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
        "plt.title(\"Feature Importance Score\", size = 20)\n",
        "plt.xlabel(\"Features/ Variables\", size = 16, color = \"black\")\n",
        "plt.ylabel(\"Importance Score\", size = 16, color = \"black\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bLgH4jMj7RHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature importance analysis reveals large variation between the input variables in terms of how important they are in predicting student performance. In particular variables G1 and G2 are strong predictors for student performance. This is because these variables are students grades from first and second term respectively, thus will be effective predictors for a students final grade. \n",
        "\n",
        "However only keeping variables G1 and G2 would overly shrink the training set, risking overfitting. Therefore only input variables that scored higher than 25 were kept in the training set. This left the variables \"Medu\", \"higher\", \"studytime\", \"failures\", \"absences\", \"G1\", \"G2\". "
      ],
      "metadata": {
        "id": "qqqNPk7E7jPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.drop(columns = [\"school\", \"sex\", \"age\", \"address\", \"famsize\", \"Pstatus\", \"Fedu\", \"Mjob\", \"Fjob\", \"reason\", \"guardian\", \"traveltime\", \"schoolsup\", \"famsup\",\n",
        "                                  \"paid\", \"activities\", \"nursery\", \"internet\", \"romantic\", \"famrel\", \"freetime\", \"goout\", \"Dalc\", \"Walc\", \"health\"])\n",
        "\n",
        "x_test = x_test.drop(columns = [\"school\", \"sex\", \"age\", \"address\", \"famsize\", \"Pstatus\", \"Fedu\", \"Mjob\", \"Fjob\", \"reason\", \"guardian\", \"traveltime\", \"schoolsup\", \"famsup\",\n",
        "                                  \"paid\", \"activities\", \"nursery\", \"internet\", \"romantic\", \"famrel\", \"freetime\", \"goout\", \"Dalc\", \"Walc\", \"health\"])"
      ],
      "metadata": {
        "id": "KvUte_M_8OB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot Encoding\n",
        "\n",
        "The data types of each variables are then investigated in order to ensure they can be supported by machine learning algorithms. Categorical variables are particularly difficult, thus often are encoded into numerical values using various techniques. Amongst the remaining variables, only \"higher\" is categorical, whilst the others are either ordinal or nominal. "
      ],
      "metadata": {
        "id": "7qzfY6Oc8Sof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.dtype"
      ],
      "metadata": {
        "id": "r3pqMaH--t3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pd.get_dummies(x_train, drop_first=True, columns = [\"higher\"])\n",
        "x_test = pd.get_dummies(x_test, drop_first=True, columns = [\"higher\"])"
      ],
      "metadata": {
        "id": "YOqgjRps-uD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling Data "
      ],
      "metadata": {
        "id": "GCaXMbzsOw66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)"
      ],
      "metadata": {
        "id": "FIuD-46SOzZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Set\n",
        "\n",
        "Finally, validation set was generated from training data to avoid overfitting and for an early evaluation of the model (Larsen et al, 1996). The size chosen for the validation set was 20%. "
      ],
      "metadata": {
        "id": "k9UHHqSN_JcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(x_train, y_train) "
      ],
      "metadata": {
        "id": "BOypIYjE_cAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline Model\n",
        "\n"
      ],
      "metadata": {
        "id": "tWTpnW-8Y602"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest Classifier\n",
        "\n",
        "A baseline model was first fitted on the training set, in order to compare performance of the neural networks. A random forest classifier was chosen, as it has been shown to be a versatile and powerful model for multiclass classifcation. \n"
      ],
      "metadata": {
        "id": "c3TtHCZNDbY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Models "
      ],
      "metadata": {
        "id": "lIh_OteVZPv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multilayer Perceptron \n",
        "\n",
        "The first model used was a Multilayer Perceptron with default parameters. Despite being a simple model, it provided a term of comparison to measure the performance of the Neural Networks. The model received a score of 0.96 when applied to the test set. "
      ],
      "metadata": {
        "id": "0sqBTyUQcEI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feedforward Neural Network \n",
        "The second model built was a 3-layer feedforward neural network. The loss function used was sparse categorical crossentropy. Adam optimiser was used for this model. The activation function used was ReLu for the hidden layers and SoftMax for the output. \n",
        "The best hyperparameters were retrieved using KerasTuner. This method proved to be effective given the high number of possible combinations for the hyperparameter. The search provided the following parameters: \n",
        "\n",
        "When applied to the test set, the model scored 0.94. The accuracy score on the training set was 1.00 \n",
        "\n",
        "Explain the difference between scores on train and test sets "
      ],
      "metadata": {
        "id": "_jn2egsycLEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wide and Deep Neural Network \n",
        "The third model built was a Wide and Deep Neural Network with 3 hidden layers. \n",
        "Activation function \n",
        "Loss function \n",
        "Optimizer \n",
        "KerasTuner was used to retrieve the best hyperparameters. The search provided a learning rate of 0.001 and 769 neurons per layer. The model scored 0.98 on the test set and 1.00 accuracy on the training set. \n",
        "The higher accuracy score on the training set may be a sign of overfitting. For this reason, He initialization and LeakyReLU, a variation of the ReLU activation function, have been added. The small slopes of the LeakyReLU function prevent neurons dying during the training stages whereas He initialization ...\n",
        "\n",
        "However, these implementations have not improved the score from 0.98. The performance graph also shows the loss for both training and validation data to be highly fluctuating. This could be due to ..."
      ],
      "metadata": {
        "id": "1Xic_1ZYcWPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wide and Deep Neural Network with an embedding layer \n",
        "\n",
        "The final model built was a Wide and Deep Neural Network with an embedding layer. While OneHotEncoding represents an effective solution to handle categorical data, embedding can help red\\uce memory space and improve performance (Guo and Berkhahn, 2016). "
      ],
      "metadata": {
        "id": "VyLj77rPce8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation \n",
        "\n",
        "Presented below is reproducible code for producing all four models produced. Alongside this, the code is annotated in order to show the approach and decisions taken, along with justifications, in order to achieve our student performance predictions. "
      ],
      "metadata": {
        "id": "OntScPxBYW9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model - Random Forest Classifier"
      ],
      "metadata": {
        "id": "ySP0wKzYBw5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rnd_clf = RandomForestClassifier(max_depth = None, n_estimators = 550, criterion = 'gini')\n",
        "\n",
        "rnd_clf.fit(x_train y_train)\n",
        "\n",
        "rf_pred = rnd_clf.predict(x_test)\n",
        "rf_pred = pd.DataFrame(rf_pred)\n",
        "rf_pred\n",
        "\n",
        "# scored 0.98 on Kaggle"
      ],
      "metadata": {
        "id": "zRd2mxtwBz63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptron"
      ],
      "metadata": {
        "id": "_SslDlL-Dgqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sklearn basic MultiLayer Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "JaCSbDdCD8-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedforward Deep Neural Network"
      ],
      "metadata": {
        "id": "pSjOn9pdDk5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New search with adam optimizer\n",
        "\n",
        "#Used unscaled data \n",
        "#did the validation split through the hyperparameter search \n",
        "\n",
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(58, )))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 1-800\n",
        "  hp_units = hp.Int('units', min_value=1, max_value=800, step=64)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(units= hp_units, activation = 'relu'))\n",
        "  model.add(keras.layers.Dense(units= hp_units, activation = 'relu'))\n",
        "  model.add(keras.layers.Dense(21, activation = \"softmax\"))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "_QNF357AE6HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=50,\n",
        "                     factor=3,\n",
        "                     directory='dir')\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "RNUfIshRE6wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train_encoded_df17, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "L-BmBiRCE-91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training model using best hyperparameters \n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(x_train_encoded_df17, y_train, epochs=50, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "co6icXqsFFjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wide and Deep Neural Network"
      ],
      "metadata": {
        "id": "x0E_BI_-Do22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameter search \n",
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  input = model.add(keras.layers.Flatten(input_shape=(58, )))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 1-800\n",
        "  hp_units = hp.Int('units', min_value=1, max_value=800, step=64)\n",
        "  input = keras.layers.Input(shape= 58,)\n",
        "  hidden1 = keras.layers.Dense(units = hp_units, activation=\"relu\")(input)\n",
        "  hidden2 = keras.layers.Dense(units = hp_units, activation=\"relu\")(hidden1)\n",
        "  concat = keras.layers.Concatenate()([input,hidden2])\n",
        "  output = keras.layers.Dense(21, activation = \"softmax\")(concat)\n",
        "  model = keras.Model(inputs=[input], outputs=[output])\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "7L6vJfUgM0-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=30,\n",
        "                     factor=3,\n",
        "                     directory='dir_1',)\n",
        "\n",
        "#early stopping function\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "cI3VxchEM1JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train_encoded_df17, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "nPqtKwxfM5ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training model using best hyperparameters \n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(x_train_encoded_df17, y_train, epochs=50, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "sDv8g7LbM5wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Hyperparameter Search to Avoid Overfitting"
      ],
      "metadata": {
        "id": "r5gv_iV-N6Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new hyperparameter search \n",
        "\n",
        "## with regularisation and gradient clipping \n",
        "\n",
        "\n",
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  input = model.add(keras.layers.Flatten(input_shape=(58, )))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 1-800\n",
        "  hp_units = hp.Int('units', min_value=1, max_value=800, step=64)\n",
        "  input = keras.layers.Input(shape= 58,)\n",
        "  hidden1 = keras.layers.Dense(units = hp_units, kernel_initializer = \"he_normal\")(input)\n",
        "  hidden1out = keras.layers.LeakyReLU(alpha = 0.2) (hidden1)\n",
        "  hidden2 = keras.layers.Dense(units = hp_units, kernel_initializer = \"he_normal\")(hidden1out)\n",
        "  hidden2out = keras.layers.LeakyReLU(alpha = 0.2) (hidden2)\n",
        "  output = keras.layers.Dense(21, activation = \"softmax\")(hidden2out)\n",
        "  model = keras.Model(inputs=[input], outputs=[output])\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "w8rIi6mzN-NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=40,\n",
        "                     factor=3,\n",
        "                     directory='dir_2',)\n",
        "\n",
        "#early stopping function\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "ItMjQ3v3OA8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train_encoded_df17, y_train, epochs=70, validation_split=0.2, callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "SNm94e9IOFyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training model using best hyperparameters \n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(x_train_encoded_df17, y_train, epochs=70, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "A6pydyUYOIyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wide and Deep Neural Network with Text Embedding Layers"
      ],
      "metadata": {
        "id": "00z12twBDrc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Discussion\n",
        "\n",
        "How did the models perform? \n",
        "\n",
        "\n",
        "\n",
        "Insert summary table of train/test performance and parameters for each model \n",
        "\n",
        "Table created here: https://docs.google.com/document/d/1Dlgx9VmFhBQXaAt7JKBXxncLi1s_48yVECCXrEAkjZE/edit?usp=sharing\n"
      ],
      "metadata": {
        "id": "rCiincJeFYuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier"
      ],
      "metadata": {
        "id": "pXUHTqDoFc4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptron"
      ],
      "metadata": {
        "id": "zzfHGbD7FhLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_preds = clf.predict(x_test_encoded_df17) \n",
        "mlp_preds = pd.DataFrame(mlp_preds)\n",
        "mlp_preds.to_csv('mlp_preds.csv')\n",
        "files.download('mlp_preds.csv')\n",
        "\n",
        "#this scored 0.96 on Kaggle "
      ],
      "metadata": {
        "id": "FKZbbU5-ZG1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedforward Deep Neural Network"
      ],
      "metadata": {
        "id": "d3cSKXd_FhWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_hps.get('learning_rate'), best_hps.get('units')) "
      ],
      "metadata": {
        "id": "IFA6dsZGF0SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5)) \n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)"
      ],
      "metadata": {
        "id": "KD-xTKlPF0-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting model to test set \n",
        "\n",
        "hp_preds = model.predict(x_test_encoded_df17)\n",
        "\n",
        "hp_preds = pd.DataFrame(hp_preds)\n",
        "hp_preds.to_csv('hp_preds.csv')\n",
        "files.download('hp_preds.csv')\n",
        "\n",
        "# results - 0.98 on Kaggle"
      ],
      "metadata": {
        "id": "iXstca4WF6D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wide and Deep Neural Network"
      ],
      "metadata": {
        "id": "1SLSoq3QFhek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieving the best hyperparameters selected\n",
        "\n",
        "print(best_hps.get('learning_rate'), best_hps.get('units')) "
      ],
      "metadata": {
        "id": "fpWdQRW4MTpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#observing performance \n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5)) \n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)"
      ],
      "metadata": {
        "id": "MuFm-ApTMT2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting model to test set \n",
        "\n",
        "wd_search_preds = model.predict(x_test_encoded_df17)\n",
        "\n",
        "wd_search_preds = pd.DataFrame(wd_search_preds)\n",
        "wd_search_preds.to_csv('wd_search_preds.csv')\n",
        "files.download('wd_search_preds.csv')\n",
        "\n",
        "#0.98 on Kaggle "
      ],
      "metadata": {
        "id": "pRd--meqMT7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Hyperparameter Search to Avoid Overfitting"
      ],
      "metadata": {
        "id": "zKBk_TJdNZ27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieving the best hyperparameters selected\n",
        "\n",
        "print(best_hps.get('learning_rate'), best_hps.get('units'))"
      ],
      "metadata": {
        "id": "oP0MqISVN2WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#observing performance \n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5)) \n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)"
      ],
      "metadata": {
        "id": "Thc0OgT2OL8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting model to test set \n",
        "\n",
        "wd_search_preds_2 = model.predict(x_test_encoded_df17)\n",
        "\n",
        "wd_search_preds_2 = pd.DataFrame(wd_search_preds_2)\n",
        "wd_search_preds_2.to_csv('wd_search_preds_2.csv')\n",
        "files.download('wd_search_preds_2.csv')\n",
        "\n",
        "#0.98 on Kaggle"
      ],
      "metadata": {
        "id": "6DJwKEofORsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wide and Deep Neural Network with text embedding layer \n",
        "\n",
        "The final model created was a Wide and Deep Neural Network with an embedding layer. \n"
      ],
      "metadata": {
        "id": "UtLT1mn9OEvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Keras\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Standard ML stuff\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD, FastICA\n",
        "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
        "\n",
        "# Oversampling of minority class 'Churn customers'\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "MoL-1mmIsOGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieve train and test sets \n",
        "path = \"/content/drive/MyDrive/test_task2.csv\"\n",
        "test = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "hSToEFQ2Uz_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/train_task2.csv\"\n",
        "train = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "Guo4pdEFU-0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.dtypes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDQMrJsuvpNb",
        "outputId": "6bcd7e6d-4456-40ef-dd76-4192914ca34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "school        float64\n",
              "sex           float64\n",
              "age           float64\n",
              "address       float64\n",
              "famsize       float64\n",
              "Pstatus       float64\n",
              "Medu          float64\n",
              "Fedu          float64\n",
              "Mjob          float64\n",
              "Fjob          float64\n",
              "reason        float64\n",
              "guardian      float64\n",
              "traveltime    float64\n",
              "studytime     float64\n",
              "failures      float64\n",
              "schoolsup     float64\n",
              "famsup        float64\n",
              "paid          float64\n",
              "activities    float64\n",
              "nursery       float64\n",
              "higher        float64\n",
              "internet      float64\n",
              "romantic      float64\n",
              "famrel        float64\n",
              "freetime      float64\n",
              "goout         float64\n",
              "Dalc          float64\n",
              "Walc          float64\n",
              "health        float64\n",
              "absences      float64\n",
              "G1            float64\n",
              "G2            float64\n",
              "Grade         float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2']\n",
        "target_col = train['Grade']\n",
        "ignored_cols = ['id']\n",
        "categorical_cols = train.select_dtypes(include='object').columns\n",
        "categorical_cols = [col for col in categorical_cols if col not in target_col]"
      ],
      "metadata": {
        "id": "Ews9c6j0rXlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "for col in categorical_cols:\n",
        "    train[col] = LabelEncoder().fit_transform(train[col])"
      ],
      "metadata": {
        "id": "o7bmbFgirv6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[numeric_cols] = StandardScaler().fit_transform(train[numeric_cols])"
      ],
      "metadata": {
        "id": "IkGT9Be8sBAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=3)\n",
        "_X = pca.fit_transform(train[numeric_cols + categorical_cols])\n",
        "pca_data = pd.DataFrame(_X, columns=[\"PCA1\", \"PCA2\", \"PCA3\"])\n",
        "train[[\"PCA1\", \"PCA2\", \"PCA3\"]] = pca_data\n",
        "\n",
        "fica = FastICA(n_components=3)\n",
        "_X = fica.fit_transform(train[numeric_cols + categorical_cols])\n",
        "fica_data = pd.DataFrame(_X, columns=[\"FICA1\", \"FICA2\", \"FICA3\"])\n",
        "train[[\"FICA1\", \"FICA2\", \"FICA3\"]] = fica_data\n",
        "\n",
        "tsvd = TruncatedSVD(n_components=3)\n",
        "_X = tsvd.fit_transform(train[numeric_cols + categorical_cols])\n",
        "tsvd_data = pd.DataFrame(_X, columns=[\"TSVD1\", \"TSVD2\", \"TSVD3\"])\n",
        "train[[\"TSVD1\", \"TSVD2\", \"TSVD3\"]] = tsvd_data\n",
        "\n",
        "grp = GaussianRandomProjection(n_components=3)\n",
        "_X = grp.fit_transform(train[numeric_cols + categorical_cols])\n",
        "grp_data = pd.DataFrame(_X, columns=[\"GRP1\", \"GRP2\", \"GRP3\"])\n",
        "train[[\"GRP1\", \"GRP2\", \"GRP3\"]] = grp_data\n",
        "\n",
        "srp = SparseRandomProjection(n_components=3)\n",
        "_X = srp.fit_transform(train[numeric_cols + categorical_cols])\n",
        "srp_data = pd.DataFrame(_X, columns=[\"SRP1\", \"SRP2\", \"SRP3\"])\n",
        "train[[\"SRP1\", \"SRP2\", \"SRP3\"]] = srp_data\n",
        "\n",
        "#tsne = TSNE(n_components=3)\n",
        "#_X = tsne.fit_transform(telcom[numeric_cols + categorical_cols])\n",
        "#tsne_data = pd.DataFrame(_X, columns=[\"TSNE1\", \"TSNE2\", \"TSNE3\"])\n",
        "#telcom[[\"TSNE1\", \"TSNE2\", \"TSNE3\"]] = tsne_data\n",
        "\n",
        "numeric_cols.extend(pca_data.columns.values)\n",
        "numeric_cols.extend(fica_data.columns.values)\n",
        "numeric_cols.extend(tsvd_data.columns.values)\n",
        "numeric_cols.extend(grp_data.columns.values)\n",
        "numeric_cols.extend(srp_data.columns.values)"
      ],
      "metadata": {
        "id": "mCym_UxL2gHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "os_smote_X, os_smote_Y = smote.fit_resample(train[numeric_cols + categorical_cols], train[target_col].values.ravel())\n",
        "\n",
        "train = pd.DataFrame(os_smote_X, columns=numeric_cols + categorical_cols)\n",
        "train['Grade'] = os_smote_Y\n",
        "print(train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "dvS6ARUp2_mW",
        "outputId": "2183ffd8-af46-49d7-e9de-c8c7f9836790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-c56b1bf1958f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minority'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos_smote_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos_smote_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_smote_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Grade'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_smote_Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([ 7, 12,  8,  9,  8,  8,  9,  9,  9, 10,\\n            ...\\n             9,  9,  9,  8,  6,  9,  8,  8,  8,  9],\\n           dtype='int64', length=20064)] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop('id', axis=1)"
      ],
      "metadata": {
        "id": "Rga6whO3sTD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()"
      ],
      "metadata": {
        "id": "zzLt6qzstsSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_COLS = numeric_cols + categorical_cols\n",
        "TARGET_COL = 'Grade'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "CLASS_WEIGHTS = {0 : 1., 1 : 2.5}"
      ],
      "metadata": {
        "id": "iUnifKX-tu1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_inputs = []\n",
        "num_inputs = []\n",
        "embeddings = []\n",
        "embedding_layer_names = []\n",
        "emb_n = 10"
      ],
      "metadata": {
        "id": "__dJzLzFuqsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h9fGsTRvCEs",
        "outputId": "7ce92019-1c66-49f5-b507-e8819cdfbc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['school',\n",
              " 'sex',\n",
              " 'address',\n",
              " 'famsize',\n",
              " 'Pstatus',\n",
              " 'Mjob',\n",
              " 'Fjob',\n",
              " 'reason',\n",
              " 'guardian',\n",
              " 'schoolsup',\n",
              " 'famsup',\n",
              " 'paid',\n",
              " 'activities',\n",
              " 'nursery',\n",
              " 'higher',\n",
              " 'internet',\n",
              " 'romantic']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding for categorical features\n",
        "for col in categorical_cols:\n",
        "    _input = layers.Input(shape=[17], name=col)\n",
        "    _embed = layers.Embedding(train[col].max() + 1, emb_n, name=col+'_emb')(_input)\n",
        "    cat_inputs.append(_input)\n",
        "    embeddings.append(_embed)\n",
        "    embedding_layer_names.append(col+'_emb')\n",
        "    \n",
        "# Simple inputs for the numeric features\n",
        "for col in numeric_cols:\n",
        "    numeric_input = layers.Input(shape=(30, ), name=col)\n",
        "    num_inputs.append(numeric_input)\n",
        "    \n",
        "# Merge the numeric inputs\n",
        "merged_num_inputs = layers.concatenate(num_inputs)\n",
        "#numeric_dense = layers.Dense(31, activation='relu')(merged_num_inputs)\n",
        "\n",
        "# Merge embedding and use a Droput to prevent overfittting\n",
        "merged_inputs = layers.concatenate(embeddings)\n",
        "spatial_dropout = layers.SpatialDropout1D(0.2)(merged_inputs)\n",
        "flat_embed = layers.Flatten()(spatial_dropout)\n",
        "\n",
        "# Merge embedding and numeric features\n",
        "all_features = layers.concatenate([flat_embed, merged_num_inputs])\n",
        "\n",
        "# MLP for classification\n",
        "x = layers.Dropout(0.2)(layers.Dense(100, activation='relu')(all_features))\n",
        "x = layers.Dropout(0.2)(layers.Dense(50, activation='relu')(x))\n",
        "x = layers.Dropout(0.2)(layers.Dense(25, activation='relu')(x))\n",
        "x = layers.Dropout(0.2)(layers.Dense(15, activation='relu')(x))\n",
        "\n",
        "# Final model\n",
        "output = layers.Dense(21, activation='sigmoid')(x)\n",
        "model = models.Model(inputs=cat_inputs + num_inputs, outputs=output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Pl1p7ZmWur8i",
        "outputId": "bdbf3b15-6a56-49e2-ca8a-c44c2cf5fcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-56792a6577eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m                   for t in tf.nest.flatten(inputs)]):\n\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 230\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       raise ValueError(\n\u001b[0;32m-> 1050\u001b[0;31m           \u001b[0;34mf'The name \"{name}\" is used {all_names.count(name)} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m           'times in the model. All layer names should be unique.')\n\u001b[1;32m   1052\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnetwork_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The name \"school\" is used 2 times in the model. All layer names should be unique."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Q9_Tnnxtnt",
        "outputId": "77b6ae20-0827-4eed-debc-a9ec3939622b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " school (InputLayer)            [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " sex (InputLayer)               [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " address (InputLayer)           [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " famsize (InputLayer)           [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " Pstatus (InputLayer)           [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " Mjob (InputLayer)              [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " Fjob (InputLayer)              [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " reason (InputLayer)            [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " guardian (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " schoolsup (InputLayer)         [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " famsup (InputLayer)            [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " paid (InputLayer)              [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " activities (InputLayer)        [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " nursery (InputLayer)           [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " higher (InputLayer)            [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " internet (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " romantic (InputLayer)          [(None, 17)]         0           []                               \n",
            "                                                                                                  \n",
            " school_emb (Embedding)         (None, 17, 10)       20          ['school[0][0]']                 \n",
            "                                                                                                  \n",
            " sex_emb (Embedding)            (None, 17, 10)       20          ['sex[0][0]']                    \n",
            "                                                                                                  \n",
            " address_emb (Embedding)        (None, 17, 10)       20          ['address[0][0]']                \n",
            "                                                                                                  \n",
            " famsize_emb (Embedding)        (None, 17, 10)       20          ['famsize[0][0]']                \n",
            "                                                                                                  \n",
            " Pstatus_emb (Embedding)        (None, 17, 10)       20          ['Pstatus[0][0]']                \n",
            "                                                                                                  \n",
            " Mjob_emb (Embedding)           (None, 17, 10)       50          ['Mjob[0][0]']                   \n",
            "                                                                                                  \n",
            " Fjob_emb (Embedding)           (None, 17, 10)       50          ['Fjob[0][0]']                   \n",
            "                                                                                                  \n",
            " reason_emb (Embedding)         (None, 17, 10)       40          ['reason[0][0]']                 \n",
            "                                                                                                  \n",
            " guardian_emb (Embedding)       (None, 17, 10)       30          ['guardian[0][0]']               \n",
            "                                                                                                  \n",
            " schoolsup_emb (Embedding)      (None, 17, 10)       20          ['schoolsup[0][0]']              \n",
            "                                                                                                  \n",
            " famsup_emb (Embedding)         (None, 17, 10)       20          ['famsup[0][0]']                 \n",
            "                                                                                                  \n",
            " paid_emb (Embedding)           (None, 17, 10)       20          ['paid[0][0]']                   \n",
            "                                                                                                  \n",
            " activities_emb (Embedding)     (None, 17, 10)       20          ['activities[0][0]']             \n",
            "                                                                                                  \n",
            " nursery_emb (Embedding)        (None, 17, 10)       20          ['nursery[0][0]']                \n",
            "                                                                                                  \n",
            " higher_emb (Embedding)         (None, 17, 10)       20          ['higher[0][0]']                 \n",
            "                                                                                                  \n",
            " internet_emb (Embedding)       (None, 17, 10)       20          ['internet[0][0]']               \n",
            "                                                                                                  \n",
            " romantic_emb (Embedding)       (None, 17, 10)       20          ['romantic[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 17, 170)      0           ['school_emb[0][0]',             \n",
            "                                                                  'sex_emb[0][0]',                \n",
            "                                                                  'address_emb[0][0]',            \n",
            "                                                                  'famsize_emb[0][0]',            \n",
            "                                                                  'Pstatus_emb[0][0]',            \n",
            "                                                                  'Mjob_emb[0][0]',               \n",
            "                                                                  'Fjob_emb[0][0]',               \n",
            "                                                                  'reason_emb[0][0]',             \n",
            "                                                                  'guardian_emb[0][0]',           \n",
            "                                                                  'schoolsup_emb[0][0]',          \n",
            "                                                                  'famsup_emb[0][0]',             \n",
            "                                                                  'paid_emb[0][0]',               \n",
            "                                                                  'activities_emb[0][0]',         \n",
            "                                                                  'nursery_emb[0][0]',            \n",
            "                                                                  'higher_emb[0][0]',             \n",
            "                                                                  'internet_emb[0][0]',           \n",
            "                                                                  'romantic_emb[0][0]']           \n",
            "                                                                                                  \n",
            " spatial_dropout1d (SpatialDrop  (None, 17, 170)     0           ['concatenate_1[0][0]']          \n",
            " out1D)                                                                                           \n",
            "                                                                                                  \n",
            " age (InputLayer)               [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " Medu (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " Fedu (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " traveltime (InputLayer)        [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " studytime (InputLayer)         [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " failures (InputLayer)          [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " famrel (InputLayer)            [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " freetime (InputLayer)          [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " goout (InputLayer)             [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " Dalc (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " Walc (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " health (InputLayer)            [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " absences (InputLayer)          [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " G1 (InputLayer)                [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " G2 (InputLayer)                [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " PCA1 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " PCA2 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " PCA3 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " FICA1 (InputLayer)             [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " FICA2 (InputLayer)             [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " FICA3 (InputLayer)             [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " TSVD1 (InputLayer)             [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " TSVD2 (InputLayer)             [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " TSVD3 (InputLayer)             [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " GRP1 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " GRP2 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " GRP3 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " SRP1 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " SRP2 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " SRP3 (InputLayer)              [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 2890)         0           ['spatial_dropout1d[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 900)          0           ['age[0][0]',                    \n",
            "                                                                  'Medu[0][0]',                   \n",
            "                                                                  'Fedu[0][0]',                   \n",
            "                                                                  'traveltime[0][0]',             \n",
            "                                                                  'studytime[0][0]',              \n",
            "                                                                  'failures[0][0]',               \n",
            "                                                                  'famrel[0][0]',                 \n",
            "                                                                  'freetime[0][0]',               \n",
            "                                                                  'goout[0][0]',                  \n",
            "                                                                  'Dalc[0][0]',                   \n",
            "                                                                  'Walc[0][0]',                   \n",
            "                                                                  'health[0][0]',                 \n",
            "                                                                  'absences[0][0]',               \n",
            "                                                                  'G1[0][0]',                     \n",
            "                                                                  'G2[0][0]',                     \n",
            "                                                                  'PCA1[0][0]',                   \n",
            "                                                                  'PCA2[0][0]',                   \n",
            "                                                                  'PCA3[0][0]',                   \n",
            "                                                                  'FICA1[0][0]',                  \n",
            "                                                                  'FICA2[0][0]',                  \n",
            "                                                                  'FICA3[0][0]',                  \n",
            "                                                                  'TSVD1[0][0]',                  \n",
            "                                                                  'TSVD2[0][0]',                  \n",
            "                                                                  'TSVD3[0][0]',                  \n",
            "                                                                  'GRP1[0][0]',                   \n",
            "                                                                  'GRP2[0][0]',                   \n",
            "                                                                  'GRP3[0][0]',                   \n",
            "                                                                  'SRP1[0][0]',                   \n",
            "                                                                  'SRP2[0][0]',                   \n",
            "                                                                  'SRP3[0][0]']                   \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 3790)         0           ['flatten[0][0]',                \n",
            "                                                                  'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100)          379100      ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 100)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 50)           5050        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 50)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 25)           1275        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 25)           0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 15)           390         ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 15)           0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 21)           336         ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 386,581\n",
            "Trainable params: 386,581\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_hist = model.fit(\n",
        "    x= train[FEATURE_COLS],\n",
        "    y=train[TARGET_COL],\n",
        "    validation_data=(train[FEATURE_COLS], train[TARGET_COL]),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_weight=CLASS_WEIGHTS,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "gB031vH6x_AE",
        "outputId": "85fe9b64-f623-4cc2-a1cd-d60d99b61f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-10aee212769e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS_WEIGHTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model\" expects 47 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(32, 47) dtype=float64>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGCmhOX9bXIP"
      },
      "source": [
        "# Summary and Recommendations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUtbukvjbdtJ"
      },
      "source": [
        "# References \n",
        "Guo, C., & Berkhahn, F. (2016). Entity embeddings of categorical variables. arXiv preprint arXiv:1604.06737.\n",
        "\n",
        "Larsen, J., Hansen, L. K., Svarer, C., & Ohlsson, M. (1996). Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop (pp. 62-71). IEEE."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}